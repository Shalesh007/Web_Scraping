{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping using Selenium Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new .csv file that the data will be written to\n",
    "\n",
    "csv_file = open('output_scraping.csv', 'w', encoding=\"UTF-8\", newline=\"\")\n",
    "writer = csv.writer(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write header names\n",
    "writer.writerow(\n",
    "    ['url', 'link_title', 'channel', 'no_of_views', 'time_uploaded', 'comment', 'author', 'comment_posted', \n",
    "     'no_of_replies','upvotes','downvotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\aws\\envs\\sallu\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: use options instead of chrome_options\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-user-media-security=true\")\n",
    "\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\aws\\envs\\sallu\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: use options instead of chrome_options\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scraping https://www.youtube.com/\n",
      "Message: element not interactable\n",
      "  (Session info: chrome=84.0.4147.89)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "youtube_pages = \"https://www.youtube.com/\"\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "driver.get(youtube_pages)\n",
    "time.sleep(10)\n",
    "try:\n",
    "    print(\"=\" * 40)  # Shows in terminal when youtube summary page with search keyword is being scraped\n",
    "    print(\"Scraping \" + youtube_pages)\n",
    "    search = driver.find_element_by_id('search')\n",
    "    search.send_keys(\"Kishore Kumar\")    \n",
    "    driver.find_element_by_id('search-icon-legacy').click()\n",
    "    time.sleep(20)    \n",
    "    vtitle = driver.find_elements_by_id('video-title')\n",
    "    subscription = driver.find_elements_by_id('byline')\n",
    "    views = driver.find_elements_by_xpath('//div[@id=\"metadata-line\"]/span[1]')\n",
    "    posted = driver.find_elements_by_xpath('//div[@id=\"metadata-line\"]/span[2]')\n",
    "    \n",
    "    tcount = 0\n",
    "    href = []\n",
    "    title = []\n",
    "    channel = []\n",
    "    numview = []\n",
    "    postdate = []\n",
    "        \n",
    "    while tcount < 10:\n",
    "        href.append(vtitle[tcount].get_attribute('href'))\n",
    "        channel.append(subscription[tcount].get_attribute('title'))\n",
    "        title.append(vtitle[tcount].text)\n",
    "        numview.append(views[tcount].text)\n",
    "        postdate.append(posted[tcount].text)  \n",
    "        tcount = tcount +1\n",
    "    \n",
    "    # launch top ten extracted links and extract comment details\n",
    "    tcount = 0    \n",
    "    while tcount < 10:  \n",
    "        youtube_dict ={}\n",
    "        # extract comment section of top ten links\n",
    "        url = href[tcount]\n",
    "        print (url)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        try:\n",
    "            print(\"+\" * 40)  # Shows in terminal when details of a new video is being scraped\n",
    "            print(\"Scraping child links \")\n",
    "            #scroll down to load comments\n",
    "            driver.execute_script('window.scrollTo(0,390);')\n",
    "            time.sleep(15)\n",
    "            #sort by top comments\n",
    "            sort= driver.find_element_by_xpath(\"\"\"//*[@id=\"icon-label\"]\"\"\")\n",
    "            sort.click()\n",
    "            time.sleep(10)\n",
    "            topcomments =driver.find_element_by_xpath(\"\"\"//*[@id=\"menu\"]/a[1]/paper-item/paper-item-body/div[1]\"\"\")\n",
    "            topcomments.click()\n",
    "            time.sleep(10)\n",
    "            # Loads 20 comments , scroll two times to load next set of 40 comments. \n",
    "            for i in range(0,2):\n",
    "                driver.execute_script(\"window.scrollTo(0,Math.max(document.documentElement.scrollHeight,document.body.scrollHeight,document.documentElement.clientHeight))\")\n",
    "                time.sleep(10)\n",
    "            \n",
    "            #count total number of comments and set index to number of comments if less than 50 otherwise set as 50. \n",
    "            totalcomments= len(driver.find_elements_by_xpath(\"\"\"//*[@id=\"content-text\"]\"\"\"))\n",
    "         \n",
    "            if totalcomments < 50:\n",
    "                index= totalcomments\n",
    "            else:\n",
    "                index= 50 \n",
    "                \n",
    "            ccount = 0\n",
    "            while ccount < index: \n",
    "                try:\n",
    "                    comment = driver.find_elements_by_xpath('//*[@id=\"content-text\"]')[ccount].text\n",
    "                except:\n",
    "                    comment = \"\"\n",
    "                try:\n",
    "                    authors = driver.find_elements_by_xpath('//a[@id=\"author-text\"]/span')[ccount].text\n",
    "                except:\n",
    "                    authors = \"\"\n",
    "                try:\n",
    "                    comment_posted = driver.find_elements_by_xpath('//*[@id=\"published-time-text\"]/a')[ccount].text\n",
    "                except:\n",
    "                    comment_posted = \"\"\n",
    "                try:\n",
    "                    replies = driver.find_elements_by_xpath('//*[@id=\"more-text\"]')[ccount].text                    \n",
    "                    if replies ==\"View reply\":\n",
    "                        replies= 1\n",
    "                    else:\n",
    "                        replies =replies.replace(\"View \",\"\")\n",
    "                        replies =replies.replace(\" replies\",\"\")\n",
    "                except:\n",
    "                    replies = \"\"\n",
    "                try:\n",
    "                    upvotes = driver.find_elements_by_xpath('//*[@id=\"vote-count-middle\"]')[ccount].text\n",
    "                except:\n",
    "                    upvotes = \"\"\n",
    "                        \n",
    "                youtube_dict['url'] = href[tcount]\n",
    "                youtube_dict['link_title'] = title[tcount]\n",
    "                youtube_dict['channel'] = channel[tcount]\n",
    "                youtube_dict['no_of_views'] = numview[tcount]\n",
    "                youtube_dict['time_uploaded'] =  postdate[tcount]\n",
    "                youtube_dict['comment'] = comment\n",
    "                youtube_dict['author'] = authors\n",
    "                youtube_dict['comment_posted'] = comment_posted\n",
    "                youtube_dict['no_of_replies'] = replies\n",
    "                youtube_dict['upvotes'] = upvotes\n",
    "                \n",
    "                writer.writerow(youtube_dict.values())\n",
    "                ccount = ccount +1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            driver.close()\n",
    "        tcount = tcount +1 \n",
    "    print(\"Scrapping process Completed\")   \n",
    "    csv_file.close()    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
